---
title: "Analyse Factorielle Exploratoire (AFE)"
author: "Manal Derghal"
date: "`r Sys.Date()`"
output: html_document
---
# Pr√©paration de l'environnement de travail

Ce bloc de code permet de s'assurer que tous les packages R n√©cessaires √† l'analyse sont install√©s. Il v√©rifie leur pr√©sence et les installe si besoin, en prenant en compte la possibilit√© d'utiliser un d√©p√¥t de packages sp√©cifique √† EDF.

```{r}
#| results: false
required_packages <- c("polycor","psych","haven","qgraph","EGAnet","Amelia","rgl")
# Fonction pour v√©rifier si un package est install√©
is_package_installed <- function(package_name) {
  return(package_name %in% installed.packages()[, "Package"])
}

# D√©finir un d√©p√¥t personnalis√© EDF si n√©cessaire
edf_repo <- "https://si-devops-mirror.edf.fr/repository/cran.r-project.org/"
# Demander √† l'utilisateur s'il utilise un PC EDF
use_edf_repo <- tolower(readline("üíª Es-tu sur un PC EDF ? (oui/non) : ")) %in% c("oui", "o", "yes", "y")

# Installation des packages
for (pkg in required_packages) {
  if (!is_package_installed(pkg)) {
    

    message(paste("üîÑ Installation de", pkg, "..."))
    if (use_edf_repo) {
      install.packages(pkg, repos = edf_repo)
    } else {
      install.packages(pkg)
    }
  } else {
    message(paste("‚úì", pkg, "d√©j√† install√©."))
  }
}

cat("\nüéâ Tous les packages sont install√©s ou d√©j√† pr√©sents. \n")
```

# Chargement des librairies

Ce code charge les librairies R n√©cessaires pour l'ex√©cution des scripts suivants, en supprimant les messages de d√©marrage pour une sortie plus propre.

```{r setup, message=FALSE, warning=FALSE}
library(polycor)
library(psych)
library(dplyr)
library(qgraph)
library(Amelia)
library(EGAnet)
library(ggplot2)
```

# Pr√©paration et nettoyage initial des donn√©es

Ce bloc de code charge les donn√©es depuis un fichier CSV, effectue un tri al√©atoire, puis conserve uniquement le bilan le plus r√©cent pour chaque individu (cle). Il supprime √©galement les lignes contenant des valeurs incorrectes pour la variable pcs.

```{r}
# Chargement des donn√©es depuis le fichier CSV.
df <- read.csv("visites_6.csv", sep=";")

df <- as.data.frame(df)

colonnes <- c(
  'cle','date_bilan','sexe2','age3','pcs1','ct_plein','ct_j_normal','ct_j_coupure','ct_j_decale','ct_j_alterne','ct_j_nuit','ct_j_deplace',
  'ct_c_depasse','ct_c_repas','ct_c_rigueur','ct_c_pression','ct_a_apprend','ct_a_varie','ct_a_libre','ct_a_coop','ct_a_reconnu','ct_ex_psy','s_anxiete_plainte','s_fatigue_plainte','s_sommeil_plainte','ct_a_desapprouve','ct_a_emploi','ct_a_concilier',
  'ct_a_qualite','qc_ieg_perspective','qc_ieg_sens','qc_ieg_objectif','qc_ieg_suggestion','qc_ieg_ambiance','ct_c_aba_neg2','f_1an2'
)

# S√©lection des colonnes
df <- df[, colonnes]

# Renommage des colonnes
nouvelles_colonnes <- c(
  'cle','date_bilan','sexe','age','pcs','tps_plein','jour_norm','jour_coup','jour_decale','jour_alterne','travail_nuit','deplac_pro',
  'depasse_horaire','saut_repas','trop_vite','pression','apprend','varie','libre','cooperation','reconnu','psy',
  's_anxiete_plainte','s_fatigue_plainte','s_sommeil_plainte','desapprouve','peur_emploi','concilier_vie',
  'qualite_travail','perspective','sens','objectif','suggestion','ambiance','taches_diff','pas_formation'
)

df <- setNames(df, nouvelles_colonnes)


# Trie de fa√ßon al√©atoire les lignes du data frame 'df'
df <- df %>% sample_n(n())

# Tri par date de bilan pour conserver le plus r√©cent pour chaque 'cle'.
df <- df %>%
  arrange(desc(date_bilan)) %>%
  distinct(cle, .keep_all = TRUE)

# Suppression des lignes avec des valeurs de 'pcs' incorrectes (1 et 2).
df <- df[!(df$pcs %in% c(1, 2)),]
```

```{r}
dim(df)
```

```{r}
head(df[, -1])
```

# S√©lection des items pour l'analyse factorielle

Ici, nous d√©finissons et cr√©ons un nouveau jeu de donn√©es (df_items) qui contient uniquement les variables pertinentes pour l'analyse factorielle, en excluant les variables d√©mographiques et certaines plaintes.

```{r}
# D√©finit les items (colonnes) √† exclure de l'analyse.
items_to_drop <- c(
  'date_bilan',
  'age',
  'sexe',
  'pcs',
  's_anxiete_plainte',
  's_fatigue_plainte',
  's_sommeil_plainte'
)

# Cr√©e un nouveau data frame avec uniquement les items √† analyser.
df_items <- df[, !(names(df) %in% items_to_drop), drop = FALSE]

# Affiche le nombre d'items s√©lectionn√©s.
cat(paste("S√©lection de", length(df_items), "items pour l'analyse.\n"))
```



```{r}
head(df_items[,-1])
```

# Recodage et transformation des variables

Ce bloc de code effectue des transformations importantes sur les donn√©es, notamment l'inversion de l'√©chelle de certaines variables ordinales et la discr√©tisation de variables continues en utilisant des quantiles.


```{r}
# Variables ordinales 0-1 √† inverser (0->1, 1->0)
# D√©finit les variables dont les scores de 0 et 1 doivent √™tre invers√©s.
variables_ordinales_0_1_a_inverser <- c('tps_plein', 'jour_norm')

# Applique l'inversion √† chaque colonne de la liste.
for (col in variables_ordinales_0_1_a_inverser) {
  df[[col]] <- 1 - df[[col]]
}
cat("Variables ordinales 0-1 invers√©es :", paste(variables_ordinales_0_1_a_inverser, collapse = ", "), "\n")


# Variables ordinales 0-3 √† inverser (0->3, 1->2, 2->1, 3->0)
# D√©finit les variables dont les scores de 0 √† 3 doivent √™tre invers√©s.
variables_ordinales_0_3_a_inverser <- c(
    'apprend', 'varie', 'libre', 'cooperation', 'reconnu',
    'concilier_vie', 'qualite_travail', 'perspective', 'sens',
    'objectif', 'suggestion'
)

# Applique l'inversion √† chaque colonne de la liste.
for (col in variables_ordinales_0_3_a_inverser) {
  df[[col]] <- 3 - df[[col]]
}
cat("Variables ordinales 0-3 invers√©es :", paste(variables_ordinales_0_3_a_inverser, collapse = ", "), "\n")


# Transformation de 'ct_c_pression' en variable ordinale.
# Calcule les quantiles pour d√©finir les points de coupe.
quantile_breaks_pression <- quantile(df_items$pression,
                                     probs = c(0, 0.25, 0.5, 0.75, 1),
                                     na.rm = TRUE)
unique_breaks_pression <- unique(quantile_breaks_pression)

# Affiche les points de coupe.
cat("Points de coupe pour 'pression' bas√©s sur les quantiles (Min, Q1, Med, Q3, Max) :\n")
print(unique_breaks_pression)

# Discr√©tise la variable 'pression' en utilisant les quantiles.
df_items$pression <- cut(df_items$pression,
                              breaks = unique_breaks_pression,
                              include.lowest = TRUE,
                              labels = FALSE)


# Transformation de 'qc_ieg_ambiance' en variable ordinale.
# Calcule les quantiles pour d√©finir les points de coupe.
quantile_breaks_ambiance <- quantile(df_items$ambiance,
                                     probs = c(0, 0.25, 0.5, 0.75, 1),
                                     na.rm = TRUE)
unique_breaks_ambiance <- unique(quantile_breaks_ambiance)

# Affiche les points de coupe.
cat("Points de coupe pour 'ambiance' bas√©s sur les quantiles (Min, Q1, Med, Q3, Max) :\n")
print(unique_breaks_ambiance)

# Discr√©tise la variable 'ambiance' en utilisant les quantiles.
df_items$ambiance <- cut(df_items$ambiance,
                                breaks = unique_breaks_ambiance,
                                include.lowest = TRUE,
                                labels = FALSE)
```

# Comparaison des corr√©lations de Pearson et Polychoriques

Ce code compare les matrices de corr√©lation de Pearson (inadapt√©e pour les donn√©es ordinales) et polychorique (plus appropri√©e) afin de montrer l'importance d'utiliser la bonne m√©thode.


```{r}
# S√©lectionne les 10 premi√®res variables pour l'analyse.
features = df_items[10:20]
features <- na.omit(features)

# Calcule la corr√©lation de Pearson, qui traite les donn√©es comme continues.
pear_cor = cor(features)

# Trace la matrice de corr√©lation de Pearson.
cor.plot(pear_cor, numbers=T, upper=FALSE,  main = "Pearson Correlation", show.legend = FALSE)

# Calcule la corr√©lation polychorique, mieux adapt√©e aux donn√©es ordinales.
poly_cor = auto.correlate(features)

# Trace la matrice de corr√©lation polychorique.
cor.plot(poly_cor, numbers=T, upper=FALSE, main = "Polychoric Correlation", show.legend = FALSE)

# Calcule la diff√©rence absolue entre les deux matrices pour montrer la distorsion.
diff_abs = abs(poly_cor - pear_cor)

png("Images/polychoric_vs_pearson.png", width = 2000, height = 1500, res = 300, units = "px")
# Trace la matrice de la diff√©rence pour visualiser l'ampleur de l'√©cart.
cor.plot(diff_abs, numbers = T, upper = FALSE, diag = FALSE, main = "Valeur absolue de la diff√©rence")
dev.off()

cor.plot(diff_abs, numbers = T, upper = FALSE, diag = FALSE, main = "Valeur absolue de la diff√©rence")

```

# Visualisation des valeurs manquantes

Ce bloc de code g√©n√®re une carte des valeurs manquantes (missmap) pour le jeu de donn√©es final, ce qui permet de visualiser la distribution des NA.


```{r}
# D√©marre l'enregistrement d'un graphique dans un fichier PNG.
png("Images/missingnessmap.png", width = 2100, height = 1500, res = 300)

# Affiche le titre "Missing map:".
cat("Missing map:\n")

# Cr√©e et affiche la carte des donn√©es manquantes pour le data frame 'df_items'.
missmap(df_items)

# Ferme le p√©riph√©rique graphique et enregistre le fichier.
dev.off()

missmap(df_items)
```

# Analyse visuelle des corr√©lations : qgraph
Le qgraph permet de visualiser la structure de corr√©lation entre les variables, en mettant en √©vidence les liens forts et faibles. Ce bloc de code g√©n√®re ce graphique et affiche √©galement les corr√©lations les plus fortes (> 0.8).


```{r}
# Suppression des lignes NA
df_items <- na.omit(df_items)

#enlever la cle
df_items2 <- df_items[,-c(1)]

# Calcule la matrice de corr√©lation polychorique pour les variables.
poly_matrix <- auto.correlate(df_items2)

# D√©marre l'enregistrement d'un graphique dans un fichier PNG.
png("Images/qgraph.png", width = 800, height = 600, res = 100)

# Cr√©e et customise le graphique de corr√©lation (qgraph).
# Les param√®tres ajustent l'affichage des ar√™tes (corr√©lations), des couleurs,
# et la taille des labels et des n≈ìuds.
qgraph(poly_matrix,
       cut = .30,
       minimum = .30,
       details = TRUE,
       labels = names(df_items2),
       label.font = 2,
       label.cex = 1.5,
       vsize = 7
)

# Ferme le p√©riph√©rique graphique et enregistre le fichier.
dev.off()

# D√©finit un seuil pour l'affichage des corr√©lations fortes.
correlation_threshold <- 0.8

cat(paste("\n--- Corr√©lations Polychoriques >", correlation_threshold, " (valeur absolue) ---\n"))

# Parcours la matrice de corr√©lation pour trouver et afficher les paires
# de variables dont la corr√©lation (en valeur absolue) d√©passe le seuil.
var_names <- colnames(poly_matrix)
n_vars <- ncol(poly_matrix)

for (i in 1:(n_vars - 1)) {
  for (j in (i + 1):n_vars) {
    correlation_value <- poly_matrix[i, j]
    if (abs(correlation_value) > correlation_threshold) {
      cat(sprintf("Corr√©lation entre %s et %s : %.3f\n",
                  var_names[i], var_names[j], correlation_value))
    }
  }
}

qgraph(poly_matrix,
       cut = .30,
       minimum = .30,
       details = TRUE,
       labels = names(df_items2),
       label.font = 2,
       label.cex = 1.5,
       vsize = 7
)
```
```{r}
dim(df_items)
```

Suite √† l'inspection du qgraph, les variables travail_nuit et jour_decale sont supprim√©es pour √©viter des probl√®mes de colin√©arit√©.

```{r}
# Suppression des colonnes 'travail_nuit' et 'jour_decale' car elles sont trop corr√©l√©es.
df_items <- df_items[,!(names(df_items) %in% c('travail_nuit','jour_decale'))]
```

# Nettoyage des donn√©es : variance et valeurs manquantes

Ce bloc de code d√©finit et ex√©cute une fonction pour nettoyer les donn√©es en supprimant les variables qui ont une variance trop faible.

```{r}
# Fonction pour nettoyer les donn√©es en fonction de la variance et des valeurs manquantes.
clean_data <- function(df_input) {
  
  # Affiche les variances des variables.
  cat("Variance par variable :\n")
  variances <- sapply(df_input, var, na.rm = TRUE)
  print(sort(variances, decreasing = FALSE))

  # Supprime les variables avec une variance faible (seuil : 0.15).
  low_var_threshold <- 0.15
  low_var_items <- names(variances[variances < low_var_threshold])
  if (length(low_var_items) > 0) {
    df_output <- df_input[, !(names(df_input) %in% low_var_items)]
    cat(paste("Variables supprim√©es pour faible variance (<", low_var_threshold, "):", paste(low_var_items, collapse=", "), "\n"))
  } else {
    df_output <- df_input
    cat(paste("Aucune variable supprim√©e pour faible variance (<", low_var_threshold, ").\n"))
  }
  return(df_output)
}

# Cr√©ez une copie de votre data frame pour ne travailler que sur les colonnes √† nettoyer
df_to_clean <- df_items[, -1]

# Appliquez la fonction 'clean_data' √† cette copie
df_cleaned <- clean_data(df_to_clean)

# data frame nettoy√© en ajoutant la colonne de cl√© (la premi√®re colonne)
df_items <- cbind(df_items[, 1], df_cleaned)
colnames(df_items)[1] <- "cle"

```

# Suppression de variables suppl√©mentaires

Certaines variables sont manuellement retir√©es car elles ne s'int√®grent pas bien dans le mod√®le factoriel (communalit√©s basses / cross-loadings)

```{r}
items_to_drop <- c(
  'pas_formation',
  'peur_emploi',
  'desapprouve',
  'qualite_travail',
  'libre',
  'concilier_vie',
  'objectif',
  'saut_repas'
)

# Cr√©e un nouveau data frame sans les variables exclues.
df_items_removed <- df_items[, !(names(df_items) %in% items_to_drop), drop = FALSE]

# Affiche le nombre d'items restants.
cat(paste("S√©lection de", length(df_items_removed)-1, "items pour l'analyse.\n"))
```



# Nouvel aper√ßu du qgraph apr√®s nettoyage

Apr√®s le nettoyage des variables, un nouveau qgraph est g√©n√©r√© pour confirmer l'impact des suppressions sur la structure de corr√©lation.


```{r}

df_items2 <- df_items_removed[,-1]

# Calcule la matrice de corr√©lation polychorique pour les variables.
poly_matrix <- auto.correlate(df_items2)

# D√©marre l'enregistrement d'un graphique dans un fichier PNG.
png("Images/qgraph2.png", width = 800, height = 600, res = 100)

# Cr√©e et customise le graphique de corr√©lation (qgraph).
# Les param√®tres ajustent l'affichage des ar√™tes (corr√©lations), des couleurs,
# et la taille des labels et des n≈ìuds.
qgraph(poly_matrix,
       cut = .30,
       minimum = .30,
       details = TRUE,
       posCOL = "darkgreen",
       negCOL = "red",
       labels = names(df_items2),
       label.font = 2,
       label.cex = 1.5,
       vsize = 7
)

# Ferme le p√©riph√©rique graphique et enregistre le fichier.
dev.off()

qgraph(poly_matrix,
       cut = .30,
       minimum = .30,
       details = TRUE,
       posCOL = "darkgreen",
       negCOL = "red",
       labels = names(df_items2),
       label.font = 2,
       label.cex = 1.5,
       vsize = 7
)
```

# √âvaluation de la faisabilit√© de l'AFE

Avant de lancer l'analyse factorielle, il faut v√©rifier si les donn√©es sont adapt√©es. Ce bloc de code effectue les tests de KMO et de sph√©ricit√© de Bartlett, et affiche le d√©terminant de la matrice de corr√©lation.

```{r}

# Fonction pour √©valuer la faisabilit√© d'une analyse factorielle exploratoire (AFE).
perform_efa_feasibility <- function(df_input) {

  # Calcule la matrice de corr√©lations polychoriques entre les variables.
  poly_matrix <- auto.correlate(df_input)

  # Effectue le test KMO pour mesurer l'ad√©quation de l'√©chantillonnage pour l'AFE.
  cat("\nTest KMO :\n")
  kmo_result <- KMO(poly_matrix)
  print(kmo_result)
  if (kmo_result$MSA >= 0.7) {
    cat(" KMO global > 0.7, l'ad√©quation est bonne.\n")
  } else if (kmo_result$MSA >= 0.6) {
    cat(" KMO global entre 0.6 et 0.7, l'ad√©quation est acceptable.\n")
  } else {
    warning("KMO global < 0.6, la faisabilit√© de l'AFE est discutable.")
  }

  # Effectue le test de sph√©ricit√© de Bartlett pour v√©rifier si les variables sont corr√©l√©es.
  cat("\nTest de Bartlett :\n")
  n_obs <- nrow(df_input)
  bartlett_result <- cortest.bartlett(poly_matrix, n = n_obs)
  print(bartlett_result)
  if (bartlett_result$p.value < 0.05) {
    cat("Test de Bartlett significatif (p < 0.05), les donn√©es sont corr√©l√©es.\n")
  } else {
    warning("Test de Bartlett non significatif (p >= 0.05), les variables ne semblent pas assez corr√©l√©es pour une AFE.")
  }

  # Affiche le d√©terminant de la matrice de corr√©lation
  cat("\nD√©terminant : \n")
  print(det(poly_matrix))

  return(poly_matrix)
}

poly_matrix <- perform_efa_feasibility(df_items2)
```

# D√©termination du nombre de facteurs
Cette √©tape utilise le Scree Plot (pour identifier le "coude") .

```{r}
# Fonction pour d√©terminer le nombre de facteurs √† retenir pour l'AFE.
determine_n_factors <- function(poly_matrix) {
  
  # D√©marre l'enregistrement du Scree Plot dans un fichier PNG.
  output_filename_scree_parallel <- paste0("images/scree_analysis", ".png")
  png(output_filename_scree_parallel, width = 2000, height = 1500, res = 300, units = "px")
  
  # G√©n√®re un Scree Plot pour visualiser les valeurs propres et trouver le "coude".
  cat("G√©n√©ration du Scree Plot (bas√© sur la matrice polychorique)...\n")
  scree(poly_matrix, factors = FALSE, pc = TRUE, hline = "-1", main = paste(""))
  
  # Ferme le p√©riph√©rique graphique et enregistre le fichier.
  dev.off()
  
  scree(poly_matrix, factors = FALSE, pc = TRUE, hline = "-1", main = paste(""))

}

determine_n_factors(poly_matrix)

```

```{r}
head(df_items2)
```

# Ex√©cution de l'Analyse Factorielle Exploratoire (AFE)

Ce bloc de code ex√©cute l'AFE en utilisant la matrice de corr√©lation polychorique. Il utilise la m√©thode d'extraction pa (Principle Axis Factoring) et une rotation oblique promax, qui est appropri√©e lorsque l'on s'attend √† ce que les facteurs soient corr√©l√©s. Il g√©n√®re aussi des graphiques des r√©sidus et des diagrammes de facteurs.

```{r}
# Fonction pour ex√©cuter l'Analyse Factorielle Exploratoire (AFE).
perform_efa <- function(poly_matrix, df_raw_data, n_obs, n_factors) {

  # Ex√©cute l'AFE en utilisant la matrice de corr√©lation polychorique.
  # Les param√®tres sp√©cifient le nombre de facteurs, la m√©thode d'extraction (pa)
  # et la rotation oblique (promax).
  efa_model <- fa(
    r = poly_matrix,
    nfactors = n_factors,
    n.obs = n_obs,
    rotate = "varimax",
    fm = "pa",
    residuals = T,
    SMC=T,
    warnings=T
    )

  # Affiche les saturations (loadings) des variables sur les facteurs.
  cat(paste("R√©sultats de l'AFE :\n"))
  print.psych(efa_model, cut = 0.3, sort = TRUE)

  # Affiche la matrice de corr√©lation entre les facteurs (Phi matrix).
  # Cela n'est pertinent que pour une rotation oblique.
  cat(paste("\nCorr√©lations entre les facteurs (Phi matrix):\n"))
  if (!is.null(efa_model$Phi)) {
    print(round(efa_model$Phi, 2))
  } else {
    cat("Note: La rotation Varimax suppose des facteurs non corr√©l√©s. La matrice Phi n'est pas informative.\n")
  }

  resd = residuals(efa_model, diag = FALSE, na.rm = TRUE)
  print(resd)
  # Enregistre l'histogramme des r√©sidus dans un fichier PNG.
  png("Images/residuals.png", width = 2000, height = 1500, res = 300, units = "px")
  hist(resd, 
       col = "red", 
       main = "R√©sidus",
       xlab = "Valeurs des r√©sidus",
       cex.main = 2,
       cex.lab = 1.5,
       cex.axis = 1.5
  )
  dev.off()
  hist(resd, col = "red", main = paste("Residuals", xlab = "Residuals"))
  
  # Cr√©e et enregistre le diagramme des facteurs dans un fichier PDF.
  pdf("Images/fadiagram.pdf", height=10, width=20)
  fa.diagram(efa_model,
              sort = TRUE,
              digits = 2,
              main = "",
              cut = 0.20
  )
  dev.off()
  fa.diagram(efa_model,
              sort = TRUE,
              digits = 2,
              main = "",
              cut = 0.20
  )

  # Retourne l'objet mod√®le complet.
  return(efa_model)
}

# D√©finit le nombre de facteurs choisis pour l'analyse.
n_factors_chosen <- 2

cat(paste("\n==> Nombre de facteurs CHOISI :", n_factors_chosen, "\n"))

# Calcule la matrice de corr√©lations polychoriques √† partir des donn√©es.
poly_matrix <- auto.correlate(df_items2)

# Ex√©cute la fonction d'AFE avec les param√®tres choisis et stocke le r√©sultat.
efa_model <- perform_efa(poly_matrix, df_items2, nrow(df_items2), n_factors_chosen)
```

# Interpr√©tation des facteurs et √©valuation de la fiabilit√©
Ce bloc de code identifie les items qui composent chaque facteur, en se basant sur les saturations (loadings), et calcule l'alpha de Cronbach pour √©valuer la coh√©rence interne de chaque facteur.

```{r}
interpret_and_reliability <- function(efa_model, df_input) {
  cat("NOTE : Les listes d'items ci-dessous sont bas√©es sur l'assignation unique de chaque item\n")
  cat("au facteur o√π il a le chargement le plus √©lev√© (en valeur absolue >= 0.3).\n")

  # Extraire les loadings
  loadings <- as.data.frame(unclass(efa_model$loadings))

  # Initialisation
  factor_items_list <- vector("list", ncol(loadings))
  names(factor_items_list) <- paste0("PA", 1:ncol(loadings))

  # Assignation unique : chaque item au facteur de son loading le plus √©lev√© (valeur absolue)
  for (item_name in rownames(loadings)) {
    abs_loadings <- abs(loadings[item_name, ])
    max_idx <- which.max(abs_loadings)
    if (abs_loadings[max_idx] >= 0.3) {
      factor_items_list[[max_idx]] <- c(factor_items_list[[max_idx]], item_name)
    }
  }

  # Affichage des items par facteur avec tri par magnitude du loading
  for (i in 1:length(factor_items_list)) {
    items <- factor_items_list[[i]]
    cat(paste0("\nItems assign√©s √† Facteur ", i, ":\n"))
    if (length(items) > 0) {
      load_vals <- loadings[items, i]
      sorted_idx <- order(abs(load_vals), decreasing = TRUE)
      sorted_items <- paste0(items, " (", round(load_vals, 3), ")")
      print(sorted_items[sorted_idx])
    } else {
      cat("Aucun item assign√©.\n")
    }
  }

  # Calcul de la fiabilit√© (alpha de Cronbach)
  cat("\n--- Calcul de la fiabilit√© (Alpha de Cronbach) pour chaque facteur ---\n")
  cat("Note : L'Alpha de Cronbach est calcul√© √† partir de la matrice de corr√©lations polychoriques, ce qui est plus appropri√© pour les donn√©es ordinales.\n")
  reliability_results <- list()
  for (i in 1:length(factor_items_list)) {
    items <- factor_items_list[[i]]
    if (length(items) > 1) {
      
      # Calcule la matrice de corr√©lations polychoriques pour les items du facteur
      polychoric_matrix <- auto.correlate(df_input[, items, drop = FALSE])
      
      # Calcule l'alpha de Cronbach √† partir de la matrice polychorique
      alpha_result <- psych::alpha(polychoric_matrix, check.keys = TRUE)
      alpha_val <- alpha_result$total$std.alpha
      
      reliability_results[[paste0("Factor", i)]] <- alpha_val
      cat(paste0("Facteur ", i, " (", length(items), " items) Alpha : ", round(alpha_val, 3), "\n"))
    } else if (length(items) == 1) {
      reliability_results[[paste0("Factor", i)]] <- NA
      cat(paste0("Facteur ", i, " (1 item) : Alpha non applicable.\n"))
    } else {
      reliability_results[[paste0("Factor", i)]] <- NA
      cat(paste0("Facteur ", i, " (0 items) : Aucun item assign√© pour le calcul de fiabilit√©.\n"))
    }
  }

  cat("-----------------------------------------------------------\n")
  cat("Un alpha > 0.7 est g√©n√©ralement consid√©r√© comme acceptable/bon.\n")
  cat("Un alpha entre 0.6 et 0.7 est marginal/discutable.\n")
  cat("Un alpha < 0.6 est g√©n√©ralement consid√©r√© comme faible.\n")

  return(list(factor_items = factor_items_list, reliability = reliability_results))
}

interpretation <- interpret_and_reliability(efa_model, df_items2)

```


# Visualisation des variables sur les facteurs

Ce code g√©n√®re un graphique de dispersion des variables dans l'espace des facteurs 1 et 2, ce qui facilite l'interpr√©tation des facteurs en montrant visuellement quels items contribuent le plus √† chaque facteur, et compare avec la solution sans rotation.

```{r}
efa_unrot <- fa(r = poly_matrix, nfactors = n_factors_chosen, rotate = "none", fm = "pa")
efa_rotobl <- fa(r = poly_matrix, nfactors = n_factors_chosen, rotate = "promax", fm = "pa")

# --- Graphique de la solution non-rotat√©e ---
if (!is.null(efa_unrot) && !is.null(efa_unrot$loadings)) {
  factor.plot(efa_unrot, 
              labels = rownames(efa_unrot$loadings), 
              choose = c(1, 2),
              main = "Solution non-rotat√©e",
              xlab = paste0("PA1 (", round(efa_unrot$Vaccounted[2, 1] * 100, 1), " %)"),
              ylab = paste0("PA2 (", round(efa_unrot$Vaccounted[2, 2] * 100, 1), " %)")
  )
}

# --- Graphique de la solution rotat√©e Varimax ---
if (!is.null(efa_model) && !is.null(efa_model$loadings)) {
  factor.plot(efa_model, 
              labels = rownames(efa_model$loadings), 
              choose = c(1, 2),
              main = "Rotation oblique (varimax)",
              xlab = paste0("PA1 (", round(efa_model$Vaccounted[2, 1] * 100, 1), " %)"),
              ylab = paste0("PA2 (", round(efa_model$Vaccounted[2, 2] * 100, 1), " %)")
  )
}
```








